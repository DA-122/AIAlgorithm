{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NiN\n",
    "## 主要贡献：\n",
    "LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。 AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。\n",
    "\n",
    "但是卷积层简单的展平再后接全连接层可能会完全放弃表征的空间结构，损失空间信息。NiN提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机 ($ 1 \\times 1$ 卷积)，相当于对前一层的所有feature map（输入通道）的线性组合（所以可以看作是全连接），再加上ReLU提供非线性变换。\n",
    "\n",
    "\n",
    "NiN和AlexNet之间的一个显著区别是**NiN完全取消了全连接层**。 相反，NiN使用一个NiN块，其**输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer）**，每个通道经池化后成为一个特征。\n",
    "NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。\n",
    "\n",
    "## $1 \\times 1$ 卷积的作用\n",
    "1. 取代全连接层：不损失空间信息，同时显著减少参数量\n",
    "2. 在卷积层中起到调整通道数的作用。\n",
    "\n",
    "![](./img/nin.svg#pic_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 54, 54]          11,712\n",
      "              ReLU-2           [-1, 96, 54, 54]               0\n",
      "            Conv2d-3           [-1, 96, 54, 54]           9,312\n",
      "              ReLU-4           [-1, 96, 54, 54]               0\n",
      "            Conv2d-5           [-1, 96, 54, 54]           9,312\n",
      "              ReLU-6           [-1, 96, 54, 54]               0\n",
      "         MaxPool2d-7           [-1, 96, 26, 26]               0\n",
      "            Conv2d-8          [-1, 256, 26, 26]         614,656\n",
      "              ReLU-9          [-1, 256, 26, 26]               0\n",
      "           Conv2d-10          [-1, 256, 26, 26]          65,792\n",
      "             ReLU-11          [-1, 256, 26, 26]               0\n",
      "           Conv2d-12          [-1, 256, 26, 26]          65,792\n",
      "             ReLU-13          [-1, 256, 26, 26]               0\n",
      "        MaxPool2d-14          [-1, 256, 12, 12]               0\n",
      "           Conv2d-15          [-1, 384, 12, 12]         885,120\n",
      "             ReLU-16          [-1, 384, 12, 12]               0\n",
      "           Conv2d-17          [-1, 384, 12, 12]         147,840\n",
      "             ReLU-18          [-1, 384, 12, 12]               0\n",
      "           Conv2d-19          [-1, 384, 12, 12]         147,840\n",
      "             ReLU-20          [-1, 384, 12, 12]               0\n",
      "        MaxPool2d-21            [-1, 384, 5, 5]               0\n",
      "          Dropout-22            [-1, 384, 5, 5]               0\n",
      "           Conv2d-23             [-1, 10, 5, 5]          34,570\n",
      "             ReLU-24             [-1, 10, 5, 5]               0\n",
      "           Conv2d-25             [-1, 10, 5, 5]             110\n",
      "             ReLU-26             [-1, 10, 5, 5]               0\n",
      "           Conv2d-27             [-1, 10, 5, 5]             110\n",
      "             ReLU-28             [-1, 10, 5, 5]               0\n",
      "AdaptiveAvgPool2d-29             [-1, 10, 1, 1]               0\n",
      "          Flatten-30                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 1,992,166\n",
      "Trainable params: 1,992,166\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 24.20\n",
      "Params size (MB): 7.60\n",
      "Estimated Total Size (MB): 31.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "def nin_blocks(in_channels,out_channels,kernel_size,strides,padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels,out_channels,kernel_size,strides,padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels,out_channels,kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels,out_channels,kernel_size=1),\n",
    "        nn.ReLU(),    \n",
    "    )\n",
    "\n",
    "class NiN(nn.Module):\n",
    "    def __init__(self, in_channels = 1, num_classes = 10):\n",
    "        super(NiN,self).__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nin_blocks(in_channels,96,kernel_size = 11,strides = 4,padding = 0),\n",
    "            nn.MaxPool2d(3,stride = 2),\n",
    "            nin_blocks(96, 256, kernel_size = 5,strides = 1,padding = 2),\n",
    "            nn.MaxPool2d(3,stride = 2),\n",
    "            nin_blocks(256, 384, kernel_size = 3,strides = 1,padding = 1),\n",
    "            nn.MaxPool2d(3,stride = 2),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nin_blocks(384,num_classes,kernel_size = 3, strides = 1, padding = 1),\n",
    "            # 实现全局平均池化，参数为output_size,tuple类型\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# net = NiN(3,10)\n",
    "net = NiN()\n",
    "summary(net,(1,224,224),device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Sequential output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 384, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Dropout output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Sequential output shape:\t torch.Size([1, 10, 5, 5])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in list(net.children())[0]:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start:\n",
      "Epoch 0: train accuracy = 0.1000,train loss = 2.3057; test accuracy = 0.1000, test loss = 2.3043\n",
      "Epoch 1: train accuracy = 0.1000,train loss = 2.3054; test accuracy = 0.1000, test loss = 2.3041\n",
      "Epoch 2: train accuracy = 0.1000,train loss = 2.3052; test accuracy = 0.1000, test loss = 2.3039\n",
      "Epoch 3: train accuracy = 0.1000,train loss = 2.3050; test accuracy = 0.1000, test loss = 2.3037\n",
      "Epoch 4: train accuracy = 0.1000,train loss = 2.3049; test accuracy = 0.1000, test loss = 2.3036\n",
      "Epoch 5: train accuracy = 0.1000,train loss = 2.3047; test accuracy = 0.1000, test loss = 2.3034\n",
      "Epoch 6: train accuracy = 0.1000,train loss = 2.3046; test accuracy = 0.1000, test loss = 2.3033\n",
      "Epoch 7: train accuracy = 0.1000,train loss = 2.3045; test accuracy = 0.1000, test loss = 2.3032\n",
      "Epoch 8: train accuracy = 0.1000,train loss = 2.3044; test accuracy = 0.1000, test loss = 2.3031\n",
      "Epoch 9: train accuracy = 0.1000,train loss = 2.3044; test accuracy = 0.1000, test loss = 2.3031\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# 超参数\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "lr =0.001\n",
    "\n",
    "trans = transforms.Compose({\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "})\n",
    "\n",
    "mnist_train = torchvision.datasets.FashionMNIST('../../../DataSets/',train = True ,transform= trans,download=False)\n",
    "mnist_test = torchvision.datasets.FashionMNIST('../../../DataSets/',train= False, transform= trans, download=False)\n",
    "\n",
    "train_loader = data.DataLoader(mnist_train,batch_size,shuffle= True)\n",
    "test_loader = data.DataLoader(mnist_test,shuffle= False,num_workers=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "optimzer = torch.optim.SGD(net.parameters(),lr = lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 权重初始化\n",
    "def init_weight(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(m.weight) \n",
    "net.apply(init_weight)\n",
    "\n",
    "\n",
    "\n",
    "def test(net,test_loader,loss,device):\n",
    "    net.eval()\n",
    "    num_correct = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in test_loader:\n",
    "            X,y = X.to(device),y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat,y)\n",
    "            test_loss += l.item()\n",
    "            pred = torch.argmax(y_hat,dim=1)\n",
    "            num_correct += torch.eq(pred,y).sum().item()\n",
    "    return test_loss/len(test_loader.dataset), num_correct/len(test_loader.dataset)\n",
    "\n",
    "def train(num_epochs,net,train_loader,loss,optimzer,device):\n",
    "\n",
    "    print('Training start:')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        num_correct = 0\n",
    "        train_loss = 0\n",
    "        for batch_idx, (X,y) in enumerate(train_loader):\n",
    "            optimzer.zero_grad()\n",
    "            X,y = X.to(device),y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat,y)\n",
    "            l.backward()\n",
    "            optimzer.step()\n",
    "            pred = torch.argmax(y_hat,dim = 1)\n",
    "            num_correct += torch.eq(pred,y).sum().item()\n",
    "            train_loss += l.item()\n",
    "            # print('Batch {}: train loss {:.4f},lr {}'.format(batch_idx,l.item(),optimzer.param_groups[0]['lr']))\n",
    "        train_loss = train_loss / (len(train_loader.dataset)/batch_size)\n",
    "        train_acc = num_correct / len(train_loader.dataset)\n",
    "        test_loss,test_acc = test(net,test_loader,loss,device)\n",
    "        print('Epoch {}: train accuracy = {:.4f},train loss = {:.4f}; test accuracy = {:.4f}, test loss = {:.4f}'.format(epoch,train_acc,train_loss,test_acc,test_loss))\n",
    "\n",
    "train(num_epochs, net,train_loader,loss,optimzer,device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
