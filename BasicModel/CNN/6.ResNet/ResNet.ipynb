{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "[CVPR 2016 Best Paper [Deep Residual Learning for Image Recognition]](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "## 主要贡献\n",
    "从AlexNet到GoogleNet、VGG，神经网络越变越深；效果也逐渐变好。理论而言神经网络越深，模型复杂度越高，模型能力越强。但是实验发现事实不是如此，越深的网络越容易退化（并不是过拟合或者梯度消失、爆炸）。\n",
    "\n",
    "ResNet：提出了残差连接(residual shortcut)，希望加深网络不会使原来的浅层网络退化，起码要保持和浅层网络持平（加深的几层训练成恒等变换）。\n",
    "\n",
    "如下图，假设原始输入为$ x $,希望经过block后得到的理想输出为 $f(x)$, 普通block直接去拟合$ f(x) $ ,residual block拟合残差 $f(x) - x$。 加入残差块加权运算的权重和偏置设为了0，那么$ x : f(x)$的映射就是一种恒等映射。\n",
    "\n",
    "<div align = center> <img src ='./img/residual-block.svg'></img></div>\n",
    "\n",
    "## 网络结构\n",
    "\n",
    "ResNet沿用了VGG完整的 $3 \\times 3$ 卷积层设计。 残差块里首先有2个有相同输出通道数的卷积层。 每个卷积层后接一个BN层和ReLU激活函数。 然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。 这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。 如果想改变通道数，就需要引入一个额外的 $1 \\times 1$\n",
    "卷积层来将输入变换成需要的形状后在做相加。\n",
    "\n",
    "<div align = center> <img src ='./img/resnet-block.svg'></img></div>\n",
    "\n",
    "\n",
    "<div align = center> <img src ='./img/DiffResnet.png'></img></div>\n",
    "\n",
    "\n",
    "**残差带来的一些问题**：模型参数量虽然不大，但非常占用显存。比起普通的卷积层堆叠(VGG,单路堆叠)，计算完一层输出后，就会释放掉上一层的输出，显存中只保存一层的结果。但是如果有残差连接，需要保存两个OutPut，相加完成后才会释放，因此虽然参数量小，但显存占用大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block实现\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,strides = 1):\n",
    "        super(Residual,self).__init__()\n",
    "        self.block = nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size = 3, padding = 1,stride = strides,bias=False),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.BatchNorm2d(out_channels),\n",
    "                                   nn.Conv2d(out_channels,out_channels,kernel_size = 3, padding = 1,bias=False),\n",
    "                                   nn.BatchNorm2d(out_channels),\n",
    "                                   nn.ReLU())\n",
    "        # 1*1卷积 (需要调整通道或者分辨率时)\n",
    "        if(in_channels != out_channels or strides!=1 ):\n",
    "            self.conv1_1 = nn.Conv2d(in_channels,out_channels,kernel_size= 1, stride = strides)\n",
    "        else:\n",
    "            self.conv1_1 = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.block(x)\n",
    "        if self.conv1_1:\n",
    "            x = self.conv1_1(x)\n",
    "        y +=x\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 6, 6]), torch.Size([4, 6, 3, 3]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(in_channels = 3, out_channels = 3)\n",
    "blk2 = Residual(3,6,2)\n",
    "X = torch.rand(4, 3, 6, 6)\n",
    "Y = blk(X)\n",
    "Y2 = blk2(X)\n",
    "Y.shape,Y2.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet18\n",
    "ResNet18也先用了一个 $7 \\times 7$ 输出通道 $64$ 的卷积，后接ReLU和 $3 \\times 3 $ 步长为 $ 2 $ 的最大池化层。\n",
    "\n",
    "ResNet使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块（ResNet18 每模块2个残差块）。 第一个模块的通道数同输入通道数一致。 由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。 之后的每个模块的第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。ResNet在每个卷积层后都有BN。\n",
    "\n",
    "最后接全局平均池化，接全连接层。\n",
    "\n",
    "\n",
    "<div align = center> <img src ='./img/resnet18.svg'></img></div>\n",
    "\n",
    "# ResNet50及以上\n",
    "\n",
    "ResNet50每个模块处理方式与ResNet18不同，由于模型层数深，因此ResNet提出了Bottle neck结构：残差Block先使用$ 1 \\times 1$ 卷积进行降维度（通道数），然后使用$3 \\times 3$ 卷积减半分辨率，最后再通过 $1 \\times 1$ 卷积升维度（通道数）。这样做能够减少参数量。\n",
    "\n",
    "\n",
    "<div align = center> <img src ='./img/bottle_neck.png'></img></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18 \n",
    "# bias 全设为False，因为后接了BN层\n",
    "from torchsummary import summary\n",
    "\n",
    "def resnet_block(in_channels,out_channels,num_residuals,isFirst = False):\n",
    "    blk = []\n",
    "    if isFirst:\n",
    "        for i in range(num_residuals):\n",
    "            blk.append(Residual(in_channels,out_channels))\n",
    "    else:\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0:\n",
    "                # 通道数乘二倍，分辨率减半\n",
    "                blk.append(Residual(in_channels,out_channels,strides = 2))\n",
    "            else:\n",
    "                blk.append(Residual(out_channels,out_channels))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, in_channels,num_classes):\n",
    "        super(ResNet18,self).__init__()\n",
    "        # (n,c,224,224) -> (n,64,112,112)\n",
    "        #               -> (n,64,56,56)\n",
    "        self.block1 = nn.Sequential(nn.Conv2d(in_channels,64,kernel_size=7,padding =3,stride =2,bias= False),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size= 3, padding=1, stride=2))\n",
    "        # 第一个残差块通道数、分辨率不变 (n,64,56,56)\n",
    "        self.resblock1 = resnet_block(64,64,2,isFirst=True)\n",
    "        # (n,128,28,28)\n",
    "        self.resblock2 = resnet_block(64,128,2)\n",
    "        # (n,256,14,14)\n",
    "        self.resblock3 = resnet_block(128,256,2)\n",
    "        # (n,512,7,7)\n",
    "        self.resblock4 = resnet_block(256,512,2)\n",
    "        # 全局平均池化\n",
    "        # (n,512,1,1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        # (全连接)\n",
    "        # (n,num_classes)\n",
    "        self.fc = nn.Sequential(nn.Flatten(),nn.Linear(512,num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# BottleNeck 结构\n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,strides = 1):\n",
    "        super(BottleNeck,self).__init__()\n",
    "        mid_channels = int(in_channels/2)\n",
    "        # 降维 - 减半 - 升维\n",
    "        self.block = nn.Sequential(nn.Conv2d(in_channels,mid_channels,kernel_size = 1,bias=False),\n",
    "                                   nn.BatchNorm2d(mid_channels),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(mid_channels,mid_channels,kernel_size = 3, padding = 1,stride = strides,bias=False),\n",
    "                                   nn.BatchNorm2d(mid_channels),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(mid_channels,out_channels,kernel_size = 1,bias=False),\n",
    "                                   nn.BatchNorm2d(out_channels),\n",
    "                                   nn.ReLU())\n",
    "        # 1*1卷积 (需要调整通道或者分辨率时)\n",
    "        if(in_channels != out_channels or strides!=1 ):\n",
    "            self.conv1_1 = nn.Conv2d(in_channels,out_channels,kernel_size= 1, stride = strides)\n",
    "        else:\n",
    "            self.conv1_1 = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.block(x)\n",
    "        if self.conv1_1:\n",
    "            x = self.conv1_1(x)\n",
    "        y +=x\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "              ReLU-6           [-1, 64, 56, 56]               0\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "         Residual-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "             ReLU-13           [-1, 64, 56, 56]               0\n",
      "      BatchNorm2d-14           [-1, 64, 56, 56]             128\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "         Residual-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "             ReLU-20          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-21          [-1, 128, 28, 28]             256\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "             ReLU-24          [-1, 128, 28, 28]               0\n",
      "           Conv2d-25          [-1, 128, 28, 28]           8,320\n",
      "         Residual-26          [-1, 128, 28, 28]               0\n",
      "           Conv2d-27          [-1, 128, 28, 28]         147,456\n",
      "             ReLU-28          [-1, 128, 28, 28]               0\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "           Conv2d-30          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-31          [-1, 128, 28, 28]             256\n",
      "             ReLU-32          [-1, 128, 28, 28]               0\n",
      "         Residual-33          [-1, 128, 28, 28]               0\n",
      "           Conv2d-34          [-1, 256, 14, 14]         294,912\n",
      "             ReLU-35          [-1, 256, 14, 14]               0\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "           Conv2d-37          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-38          [-1, 256, 14, 14]             512\n",
      "             ReLU-39          [-1, 256, 14, 14]               0\n",
      "           Conv2d-40          [-1, 256, 14, 14]          33,024\n",
      "         Residual-41          [-1, 256, 14, 14]               0\n",
      "           Conv2d-42          [-1, 256, 14, 14]         589,824\n",
      "             ReLU-43          [-1, 256, 14, 14]               0\n",
      "      BatchNorm2d-44          [-1, 256, 14, 14]             512\n",
      "           Conv2d-45          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-46          [-1, 256, 14, 14]             512\n",
      "             ReLU-47          [-1, 256, 14, 14]               0\n",
      "         Residual-48          [-1, 256, 14, 14]               0\n",
      "           Conv2d-49            [-1, 512, 7, 7]       1,179,648\n",
      "             ReLU-50            [-1, 512, 7, 7]               0\n",
      "      BatchNorm2d-51            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-52            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-53            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-54            [-1, 512, 7, 7]               0\n",
      "           Conv2d-55            [-1, 512, 7, 7]         131,584\n",
      "         Residual-56            [-1, 512, 7, 7]               0\n",
      "           Conv2d-57            [-1, 512, 7, 7]       2,359,296\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "      BatchNorm2d-59            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "         Residual-63            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-64            [-1, 512, 1, 1]               0\n",
      "          Flatten-65                  [-1, 512]               0\n",
      "           Linear-66                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,180,746\n",
      "Trainable params: 11,180,746\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 61.45\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 104.67\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 28, 28])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet18(3,10)\n",
    "\n",
    "summary(net,(3,224,224),device=\"cpu\")\n",
    "\n",
    "bottle_neck = BottleNeck(128,256,2)\n",
    "X = torch.rand((1,128,56,56),dtype = torch.float32)\n",
    "Y = bottle_neck(X)\n",
    "Y.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我加载了官方预训练模型的参数，发现ResNet18卷积层都没有Bias参数，这是为什么呢？\n",
    "\n",
    "因为BN求均值，完全无视卷积的bias效果，不管bias如何平移数据，BN均值都会将数据中心归零，都会是一个结果，所以卷积的bias无用，可以不加。而BN中beta的作用，就是卷积层中的bias的作用。可以理解为BN相当于是在卷积的卷积和bias两个环节中间，加了均值，方差，gamma三个环节把卷积的bias，变成了自己的beta。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "block1.0.weight           conv1.weight\n",
      "block1.1.weight           bn1.running_mean\n",
      "block1.1.bias           bn1.running_var\n",
      "block1.1.running_mean           bn1.weight\n",
      "block1.1.running_var           bn1.bias\n",
      "block1.1.num_batches_tracked           layer1.0.conv1.weight\n",
      "resblock1.0.block.0.weight           layer1.0.bn1.running_mean\n",
      "resblock1.0.block.2.weight           layer1.0.bn1.running_var\n",
      "resblock1.0.block.2.bias           layer1.0.bn1.weight\n",
      "resblock1.0.block.2.running_mean           layer1.0.bn1.bias\n",
      "resblock1.0.block.2.running_var           layer1.0.conv2.weight\n",
      "resblock1.0.block.2.num_batches_tracked           layer1.0.bn2.running_mean\n",
      "resblock1.0.block.3.weight           layer1.0.bn2.running_var\n",
      "resblock1.0.block.4.weight           layer1.0.bn2.weight\n",
      "resblock1.0.block.4.bias           layer1.0.bn2.bias\n",
      "resblock1.0.block.4.running_mean           layer1.1.conv1.weight\n",
      "resblock1.0.block.4.running_var           layer1.1.bn1.running_mean\n",
      "resblock1.0.block.4.num_batches_tracked           layer1.1.bn1.running_var\n",
      "resblock1.1.block.0.weight           layer1.1.bn1.weight\n",
      "resblock1.1.block.2.weight           layer1.1.bn1.bias\n",
      "resblock1.1.block.2.bias           layer1.1.conv2.weight\n",
      "resblock1.1.block.2.running_mean           layer1.1.bn2.running_mean\n",
      "resblock1.1.block.2.running_var           layer1.1.bn2.running_var\n",
      "resblock1.1.block.2.num_batches_tracked           layer1.1.bn2.weight\n",
      "resblock1.1.block.3.weight           layer1.1.bn2.bias\n",
      "resblock1.1.block.4.weight           layer2.0.conv1.weight\n",
      "resblock1.1.block.4.bias           layer2.0.bn1.running_mean\n",
      "resblock1.1.block.4.running_mean           layer2.0.bn1.running_var\n",
      "resblock1.1.block.4.running_var           layer2.0.bn1.weight\n",
      "resblock1.1.block.4.num_batches_tracked           layer2.0.bn1.bias\n",
      "resblock2.0.block.0.weight           layer2.0.conv2.weight\n",
      "resblock2.0.block.2.weight           layer2.0.bn2.running_mean\n",
      "resblock2.0.block.2.bias           layer2.0.bn2.running_var\n",
      "resblock2.0.block.2.running_mean           layer2.0.bn2.weight\n",
      "resblock2.0.block.2.running_var           layer2.0.bn2.bias\n",
      "resblock2.0.block.2.num_batches_tracked           layer2.0.downsample.0.weight\n",
      "resblock2.0.block.3.weight           layer2.0.downsample.1.running_mean\n",
      "resblock2.0.block.4.weight           layer2.0.downsample.1.running_var\n",
      "resblock2.0.block.4.bias           layer2.0.downsample.1.weight\n",
      "resblock2.0.block.4.running_mean           layer2.0.downsample.1.bias\n",
      "resblock2.0.block.4.running_var           layer2.1.conv1.weight\n",
      "resblock2.0.block.4.num_batches_tracked           layer2.1.bn1.running_mean\n",
      "resblock2.0.conv1_1.weight           layer2.1.bn1.running_var\n",
      "resblock2.0.conv1_1.bias           layer2.1.bn1.weight\n",
      "resblock2.1.block.0.weight           layer2.1.bn1.bias\n",
      "resblock2.1.block.2.weight           layer2.1.conv2.weight\n",
      "resblock2.1.block.2.bias           layer2.1.bn2.running_mean\n",
      "resblock2.1.block.2.running_mean           layer2.1.bn2.running_var\n",
      "resblock2.1.block.2.running_var           layer2.1.bn2.weight\n",
      "resblock2.1.block.2.num_batches_tracked           layer2.1.bn2.bias\n",
      "resblock2.1.block.3.weight           layer3.0.conv1.weight\n",
      "resblock2.1.block.4.weight           layer3.0.bn1.running_mean\n",
      "resblock2.1.block.4.bias           layer3.0.bn1.running_var\n",
      "resblock2.1.block.4.running_mean           layer3.0.bn1.weight\n",
      "resblock2.1.block.4.running_var           layer3.0.bn1.bias\n",
      "resblock2.1.block.4.num_batches_tracked           layer3.0.conv2.weight\n",
      "resblock3.0.block.0.weight           layer3.0.bn2.running_mean\n",
      "resblock3.0.block.2.weight           layer3.0.bn2.running_var\n",
      "resblock3.0.block.2.bias           layer3.0.bn2.weight\n",
      "resblock3.0.block.2.running_mean           layer3.0.bn2.bias\n",
      "resblock3.0.block.2.running_var           layer3.0.downsample.0.weight\n",
      "resblock3.0.block.2.num_batches_tracked           layer3.0.downsample.1.running_mean\n",
      "resblock3.0.block.3.weight           layer3.0.downsample.1.running_var\n",
      "resblock3.0.block.4.weight           layer3.0.downsample.1.weight\n",
      "resblock3.0.block.4.bias           layer3.0.downsample.1.bias\n",
      "resblock3.0.block.4.running_mean           layer3.1.conv1.weight\n",
      "resblock3.0.block.4.running_var           layer3.1.bn1.running_mean\n",
      "resblock3.0.block.4.num_batches_tracked           layer3.1.bn1.running_var\n",
      "resblock3.0.conv1_1.weight           layer3.1.bn1.weight\n",
      "resblock3.0.conv1_1.bias           layer3.1.bn1.bias\n",
      "resblock3.1.block.0.weight           layer3.1.conv2.weight\n",
      "resblock3.1.block.2.weight           layer3.1.bn2.running_mean\n",
      "resblock3.1.block.2.bias           layer3.1.bn2.running_var\n",
      "resblock3.1.block.2.running_mean           layer3.1.bn2.weight\n",
      "resblock3.1.block.2.running_var           layer3.1.bn2.bias\n",
      "resblock3.1.block.2.num_batches_tracked           layer4.0.conv1.weight\n",
      "resblock3.1.block.3.weight           layer4.0.bn1.running_mean\n",
      "resblock3.1.block.4.weight           layer4.0.bn1.running_var\n",
      "resblock3.1.block.4.bias           layer4.0.bn1.weight\n",
      "resblock3.1.block.4.running_mean           layer4.0.bn1.bias\n",
      "resblock3.1.block.4.running_var           layer4.0.conv2.weight\n",
      "resblock3.1.block.4.num_batches_tracked           layer4.0.bn2.running_mean\n",
      "resblock4.0.block.0.weight           layer4.0.bn2.running_var\n",
      "resblock4.0.block.2.weight           layer4.0.bn2.weight\n",
      "resblock4.0.block.2.bias           layer4.0.bn2.bias\n",
      "resblock4.0.block.2.running_mean           layer4.0.downsample.0.weight\n",
      "resblock4.0.block.2.running_var           layer4.0.downsample.1.running_mean\n",
      "resblock4.0.block.2.num_batches_tracked           layer4.0.downsample.1.running_var\n",
      "resblock4.0.block.3.weight           layer4.0.downsample.1.weight\n",
      "resblock4.0.block.4.weight           layer4.0.downsample.1.bias\n",
      "resblock4.0.block.4.bias           layer4.1.conv1.weight\n",
      "resblock4.0.block.4.running_mean           layer4.1.bn1.running_mean\n",
      "resblock4.0.block.4.running_var           layer4.1.bn1.running_var\n",
      "resblock4.0.block.4.num_batches_tracked           layer4.1.bn1.weight\n",
      "resblock4.0.conv1_1.weight           layer4.1.bn1.bias\n",
      "resblock4.0.conv1_1.bias           layer4.1.conv2.weight\n",
      "resblock4.1.block.0.weight           layer4.1.bn2.running_mean\n",
      "resblock4.1.block.2.weight           layer4.1.bn2.running_var\n",
      "resblock4.1.block.2.bias           layer4.1.bn2.weight\n",
      "resblock4.1.block.2.running_mean           layer4.1.bn2.bias\n",
      "resblock4.1.block.2.running_var           fc.weight\n",
      "resblock4.1.block.2.num_batches_tracked           fc.bias\n"
     ]
    }
   ],
   "source": [
    "# pytorch的预训练模型\n",
    "resnet18 = torch.load('./resnet18-5c106cde.pth')\n",
    "# for weight in resnet18:\n",
    "#     print(weight,resnet18[weight].data.shape)\n",
    "print(type(resnet18))\n",
    "\n",
    "params = net.state_dict()\n",
    "for weight1,weight2 in zip(params,resnet18):\n",
    "    print(weight1,'         ',weight2)\n",
    "# print(net.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual 改进\n",
    "[论文 Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf)\n",
    "<div align = center> <img src ='./img/residual_proposed.png'></img></div>\n",
    "\n",
    "后续何恺明团队对Residual Block提出了改进，如上图所示。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
