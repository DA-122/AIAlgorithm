{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "[CVPR 2017 Best Paper [Densely Connected Convolutional Networks]](https://arxiv.org/abs/1608.06993)\n",
    "\n",
    "[DenseNet 可视化](https://zhuanlan.zhihu.com/p/141178215)\n",
    "## 回顾ResNet：\n",
    "任意函数的泰勒公式将函数分解为越来越高阶的项，在0处展开如下(麦克劳林式)：\n",
    "$$f(x) = f(0) + f'(0)x + \\frac{f''(0)}{2!}x^2 + ·····$$\n",
    "而ResNet将函数展开成一个简单线性项和一个复杂非线性项，如下:\n",
    "$$f(X) = x + g(x)$$\n",
    "这里的非线性项可以看作是泰勒公式中的多项式项组合。\n",
    "\n",
    "## DenseNet思想\n",
    "DenseNet想要将f拓展成超过两部分的信息。\n",
    "$$x -> [x,f_1(x),f_2([x,f_1(x)]),·····]$$\n",
    "DenseNet与ResNet的区别在于DenseNet输出与输入做Concat,ResNet做的是加法。\n",
    "<div align = center> <img src ='./img/densenet-block.svg'></img></div>\n",
    "DenseNet最后一层与前面的所有层紧密相连,最后将这些展开式结合到全连接。\n",
    "<div align = center> <img src ='./img/densenet.svg'></img></div>\n",
    "\n",
    "## DenseBlock\n",
    "DenseBlock使用的是Residual Block的改进版本（先BN，激活再卷积）。每个DenseBlock由若干个卷积块组成，每个卷积块输出通道相同，但在前向传播中会将卷积块输入输出做通道维度的连结。卷积块的数量控制了输出通道数相对于输入通道数的增长，因此输出通道也叫做增长率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 改良的Residual block\n",
    "def conv_block(in_channels,out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)\n",
    "    )\n",
    "\n",
    "# DenseBlock输入输出分辨率相同，只是通道数改变\n",
    "# 内部每个block都进行Concat\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self,num_convs,in_channels,out_channels):\n",
    "        super(DenseBlock,self).__init__()\n",
    "        layer = []\n",
    "        for i in range (num_convs):\n",
    "            layer.append(conv_block(out_channels*i + in_channels,out_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self,X):\n",
    "        for block in self.net:\n",
    "            Y = block(X)\n",
    "            X = torch.cat((X,Y),dim = 1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出通道数 10+10+3\n",
    "blk = DenseBlock(2, 3, 10)\n",
    "X = torch.randn(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过渡层\n",
    "稠密块会带来通道数的增加，过多的稠密块会使模型过于复杂，过渡层就是为了减小通道数，来控制复杂度。\n",
    "过渡层使用 $1\\times 1$卷积核，减小通道数，同时使用$2 \\times 2$步长为2的平均池化层来减半高和宽。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过渡层 分辨率减半，通道数改变\n",
    "def transition_block(in_channels,out_channels):\n",
    "    return nn.Sequential(nn.BatchNorm2d(in_channels),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Conv2d(in_channels,out_channels,kernel_size=1),\n",
    "                         nn.AvgPool2d(kernel_size=2,stride=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_block = transition_block(23,10)\n",
    "trans_block(Y).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet网络结构\n",
    "网络结构与ResNet类似，只是将Residual Block替换成了Dense Block。仿照ResNet18，可以设置4个DenseBlock，每个Block有4个卷积层。每个卷积层的输出通道数设为32(growth_rate)。每个DenseBlock就会增加128通道。\n",
    "\n",
    "DenseBlock与DenseBlock之间加过渡层（减半分辨率和通道数）\n",
    "\n",
    "DenseNet相比起ResNet：可学习参数量少（卷积核输出维度少，多个卷积核concat成了大通道）；更加占用显存，DenseBlock中，每个卷积层的输出都需要保留，直到Block结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,472\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
      "              ReLU-6           [-1, 64, 56, 56]               0\n",
      "            Conv2d-7           [-1, 64, 56, 56]          36,928\n",
      "       BatchNorm2d-8          [-1, 128, 56, 56]             256\n",
      "              ReLU-9          [-1, 128, 56, 56]               0\n",
      "           Conv2d-10           [-1, 64, 56, 56]          73,792\n",
      "      BatchNorm2d-11          [-1, 192, 56, 56]             384\n",
      "             ReLU-12          [-1, 192, 56, 56]               0\n",
      "           Conv2d-13           [-1, 64, 56, 56]         110,656\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "           Conv2d-16           [-1, 64, 56, 56]         147,520\n",
      "       DenseBlock-17          [-1, 320, 56, 56]               0\n",
      "      BatchNorm2d-18          [-1, 320, 56, 56]             640\n",
      "             ReLU-19          [-1, 320, 56, 56]               0\n",
      "           Conv2d-20          [-1, 160, 56, 56]          51,360\n",
      "        AvgPool2d-21          [-1, 160, 28, 28]               0\n",
      "      BatchNorm2d-22          [-1, 160, 28, 28]             320\n",
      "             ReLU-23          [-1, 160, 28, 28]               0\n",
      "           Conv2d-24           [-1, 64, 28, 28]          92,224\n",
      "      BatchNorm2d-25          [-1, 224, 28, 28]             448\n",
      "             ReLU-26          [-1, 224, 28, 28]               0\n",
      "           Conv2d-27           [-1, 64, 28, 28]         129,088\n",
      "      BatchNorm2d-28          [-1, 288, 28, 28]             576\n",
      "             ReLU-29          [-1, 288, 28, 28]               0\n",
      "           Conv2d-30           [-1, 64, 28, 28]         165,952\n",
      "      BatchNorm2d-31          [-1, 352, 28, 28]             704\n",
      "             ReLU-32          [-1, 352, 28, 28]               0\n",
      "           Conv2d-33           [-1, 64, 28, 28]         202,816\n",
      "       DenseBlock-34          [-1, 416, 28, 28]               0\n",
      "      BatchNorm2d-35          [-1, 416, 28, 28]             832\n",
      "             ReLU-36          [-1, 416, 28, 28]               0\n",
      "           Conv2d-37          [-1, 208, 28, 28]          86,736\n",
      "        AvgPool2d-38          [-1, 208, 14, 14]               0\n",
      "      BatchNorm2d-39          [-1, 208, 14, 14]             416\n",
      "             ReLU-40          [-1, 208, 14, 14]               0\n",
      "           Conv2d-41           [-1, 64, 14, 14]         119,872\n",
      "      BatchNorm2d-42          [-1, 272, 14, 14]             544\n",
      "             ReLU-43          [-1, 272, 14, 14]               0\n",
      "           Conv2d-44           [-1, 64, 14, 14]         156,736\n",
      "      BatchNorm2d-45          [-1, 336, 14, 14]             672\n",
      "             ReLU-46          [-1, 336, 14, 14]               0\n",
      "           Conv2d-47           [-1, 64, 14, 14]         193,600\n",
      "      BatchNorm2d-48          [-1, 400, 14, 14]             800\n",
      "             ReLU-49          [-1, 400, 14, 14]               0\n",
      "           Conv2d-50           [-1, 64, 14, 14]         230,464\n",
      "       DenseBlock-51          [-1, 464, 14, 14]               0\n",
      "      BatchNorm2d-52          [-1, 464, 14, 14]             928\n",
      "             ReLU-53          [-1, 464, 14, 14]               0\n",
      "           Conv2d-54          [-1, 232, 14, 14]         107,880\n",
      "        AvgPool2d-55            [-1, 232, 7, 7]               0\n",
      "      BatchNorm2d-56            [-1, 232, 7, 7]             464\n",
      "             ReLU-57            [-1, 232, 7, 7]               0\n",
      "           Conv2d-58             [-1, 64, 7, 7]         133,696\n",
      "      BatchNorm2d-59            [-1, 296, 7, 7]             592\n",
      "             ReLU-60            [-1, 296, 7, 7]               0\n",
      "           Conv2d-61             [-1, 64, 7, 7]         170,560\n",
      "      BatchNorm2d-62            [-1, 360, 7, 7]             720\n",
      "             ReLU-63            [-1, 360, 7, 7]               0\n",
      "           Conv2d-64             [-1, 64, 7, 7]         207,424\n",
      "      BatchNorm2d-65            [-1, 424, 7, 7]             848\n",
      "             ReLU-66            [-1, 424, 7, 7]               0\n",
      "           Conv2d-67             [-1, 64, 7, 7]         244,288\n",
      "       DenseBlock-68            [-1, 488, 7, 7]               0\n",
      "      BatchNorm2d-69            [-1, 488, 7, 7]             976\n",
      "             ReLU-70            [-1, 488, 7, 7]               0\n",
      "AdaptiveAvgPool2d-71            [-1, 488, 1, 1]               0\n",
      "          Flatten-72                  [-1, 488]               0\n",
      "           Linear-73                   [-1, 10]           4,890\n",
      "================================================================\n",
      "Total params: 2,687,842\n",
      "Trainable params: 2,687,842\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 115.38\n",
      "Params size (MB): 10.25\n",
      "Estimated Total Size (MB): 126.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "num_convs = [4,4,4,4]\n",
    "growth_rate = 64\n",
    "num_channels = 64\n",
    "num_classes = 10\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,in_channels,num_channels,nums_class):\n",
    "        super(DenseNet,self).__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels,num_channels,kernel_size=7, padding = 3, stride = 2),\n",
    "                                  nn.BatchNorm2d(num_channels),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.MaxPool2d(kernel_size=3,stride=2,padding=1))\n",
    "        dense_blocks = []\n",
    "        for i, num_conv in enumerate(num_convs):\n",
    "            dense_blocks.append(DenseBlock(num_conv,num_channels,growth_rate))\n",
    "            num_channels+=num_conv*growth_rate\n",
    "            # 不是最后一个block，需要加过渡层\n",
    "            # //是Math.floor()\n",
    "            if i != (len(num_convs) - 1):\n",
    "                dense_blocks.append(transition_block(num_channels,num_channels//2))\n",
    "                num_channels = num_channels//2\n",
    "        # 输出尺寸为\n",
    "        self.dense = nn.Sequential(*dense_blocks)\n",
    "        self.fc = nn.Sequential(nn.BatchNorm2d(num_channels),\n",
    "                                nn.ReLU(),\n",
    "                                nn.AdaptiveAvgPool2d((1,1)),\n",
    "                                nn.Flatten(),\n",
    "                                nn.Linear(num_channels,nums_class))\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.fc(self.dense(self.conv(X)))\n",
    "        return X\n",
    "\n",
    "net = DenseNet(3,num_channels,num_classes)\n",
    "\n",
    "summary(net,(3,224,224),device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
